```markdown
# Project Files Overview

This document provides an overview of the key files in the repository along with their content for clarity and understanding.

## Root Directory

### .gitignore
```plaintext
# Python specific files

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
pip-wheel-metadata/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Virtual environments
venv/
ENV/
env/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# PyCharm settings
.idea/

# Jupyter Notebook checkpoints
.ipynb_checkpoints

# pyenv
# For a Unix/macOS environment
.python-version

# Pylint report
pylint-report.txt

# pytest
.cache

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# pytest cache
.pytest_cache/

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# pyright
pyrightconfig.json

# Python type checking
.pytype/

# End of Python specific files

# Terraform specific files

# Local .terraform directories
**/.terraform/*

# Ignore override files as they are usually used to override resources locally
*.tfstate
*.tfstate.*
terraform.tfvars
.terraform.lock.hcl

# Crash log files
crash.log

# Ignore env var files
.env

# Ignore plan files
*.tfplan

# Ignore backup files generated by Terraform
*.backup

# End of Terraform specific files

# Serverless directories
.serverless
node_modules

# Ignore virtual environments
.venv/

# AWS Lambda Layers
layers/
```

### Makefile
```plaintext
# Define bash as the shell for Makefile
SHELL := /bin/bash

# Define variables
PYTHON := python3

# ------------- Environment Setup --------------
# Target to run the set_aws_env.sh script and set environment variables
env:
    @echo "Configuring environment variables..."
    @bash set_aws_env.sh  # Call the external script to set the environment variables

# Check if environment variables are properly set
check:
    @echo "Verifying environment variables..."
    @$(PYTHON) -c "from utils import check_environment_variables; check_environment_variables()"
    @echo "All environment variables are set!"

# ------------- Terraform Commands --------------
# Initialize Terraform
terraform-init:
    @echo "Initializing Terraform..."
    @cd terraform && terraform init

# Plan Terraform Infrastructure
terraform-plan:
    @echo "Planning Terraform infrastructure..."
    @cd terraform && terraform plan \
        -var="aws_profile=$(TF_VAR_AWS_PROFILE)" \
        -var="aws_default_region=$(TF_VAR_AWS_DEFAULT_REGION)" \
        -var="environment=$(TF_VAR_ENVIRONMENT)" \
        -var="bucket_name=$(TF_VAR_BUCKET_NAME)" \
        -var="db_username=$(TF_VAR_DB_USERNAME)"

# Apply Terraform Infrastructure
terraform-apply:
    @echo "Applying Terraform infrastructure..."
    @cd terraform && terraform apply \
        -var="aws_profile=$(TF_VAR_AWS_PROFILE)" \
        -var="aws_default_region=$(TF_VAR_AWS_DEFAULT_REGION)" \
        -var="environment=$(TF_VAR_ENVIRONMENT)" \
        -var="bucket_name=$(TF_VAR_BUCKET_NAME)" \
        -var="db_username=$(TF_VAR_DB_USERNAME)"

# Capture the RDS endpoint and Secrets ARN from Terraform and store it in ~/.bashrc if it doesn't already exist else update it
capture-tf-output:
    @echo "Capturing Terraform output"
    @cd terraform && \
    RDS_ENDPOINT=$$(terraform output --raw rds_endpoint) && \
    RDS_SECRETS_ARN=$$(terraform output --raw rds_secret_arn) && \
    echo "RDS Endpoint is: $$RDS_ENDPOINT" && \
    echo "SECRETS ARN is: $$RDS_SECRETS_ARN" && \
    echo "export RDS_ENDPOINT=$$RDS_ENDPOINT" >> ~/.bashrc && \
    echo "export RDS_SECRETS_ARN=$$RDS_SECRETS_ARN" >> ~/.bashrc

# Destroy Terraform Infrastructure
terraform-destroy:
    @echo "Destroying Terraform infrastructure..."
    @cd terraform && terraform destroy \
        -var="aws_profile=$(TF_VAR_AWS_PROFILE)" \
        -var="aws_default_region=$(TF_VAR_AWS_DEFAULT_REGION)" \
        -var="environment=$(TF_VAR_ENVIRONMENT)" \
        -var="bucket_name=$(TF_VAR_BUCKET_NAME)" \
        -var="db_username=$(TF_VAR_DB_USERNAME)"

# ------------- Serverless Framework Commands --------------
# Deploy Serverless application
build-layer-pymysql:
    @echo "Building PyMySQL Lambda layer..."
    @./install_dependencies.sh
    @echo "PyMySQL layer built."

sls-deploy:
    @echo "Deploying Serverless application..."
    @cd src && serverless deploy --stage $(ENVIRONMENT) --aws-profile $(AWS_PROFILE)

sls-remove:
    @echo "Removing Serverless application..."
    @cd src && serverless remove --stage $(ENVIRONMENT) --aws-profile $(AWS_PROFILE)

# ------------- Default Setup and Cleanup --------------
# Full setup: configure environment, verify variables, initialize Terraform, plan, apply infrastructure, capture-rds-endpoint, and deploy Serverless
all: env check terraform-init terraform-plan terraform-apply capture-rds-endpoint build-layer-pymysql sls-deploy

# Clean up: destroy the Terraform-managed infrastructure and remove the Serverless application
clean: terraform-destroy serverless-remove

# Prevent make from looking for files with these names
.PHONY: env check terraform-init terraform-plan terraform-apply capture-rds-endpoint terraform-destroy build-layer-pymysql sls-deploy all clean
```

### cleanup_env.sh
```bash
#!/bin/bash

# Remove lines containing the environment variables from ~/.bashrc
sed -i '/AWS_PROFILE=/d' ~/.bashrc
sed -i '/AWS_DEFAULT_REGION=/d' ~/.bashrc
sed -i '/ENVIRONMENT=/d' ~/.bashrc
sed -i '/BUCKET_NAME=/d' ~/.bashrc
sed -i '/TF_VAR_AWS_PROFILE=/d' ~/.bashrc
sed -i '/TF_VAR_AWS_DEFAULT_REGION=/d' ~/.bashrc
sed -i '/TF_VAR_ENVIRONMENT=/d' ~/.bashrc
sed -i '/TF_VAR_BUCKET_NAME=/d' ~/.bashrc

# Unset the variables from the current session
unset AWS_PROFILE
unset AWS_DEFAULT_REGION
unset ENVIRONMENT
unset BUCKET_NAME
unset TF_VAR_AWS_PROFILE
unset TF_VAR_AWS_DEFAULT_REGION
unset TF_VAR_ENVIRONMENT
unset TF_VAR_BUCKET_NAME

# Source the ~/.bashrc to apply changes (no longer needed in this case)
source ~/.bashrc

echo "Environment variables have been removed from ~/.bashrc and unset in the current session."
```

### install_dependencies.sh
```bash
#!/bin/bash
# install_dependencies.sh

# Step 1: Install PyMySQL to the Lambda layer directory
echo "Installing PyMySQL..."
pip install "pymysql>=1.1.1" -t src/layers/pymysql/python

# Step 2: Zip the contents of the layer directory
echo "Zipping the PyMySQL layer..."
cd src/layers/pymysql || exit
zip -r ../pymysql-layer.zip python

# Step 3: Output success message
echo "PyMySQL layer built and zipped successfully."
```

### pyproject.toml
```toml
[project]
name = "data-engineering-assessment-a"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "aws-lambda-powertools>=3.0.0",
    "awswrangler>=3.9.1",
    "pymysql>=1.1.1",
    "python-decouple>=3.8",
]
```

### set_aws_env.sh
```bash
#!/bin/bash

# Request input from the user
read -p "Enter your AWS Profile: " AWS_PROFILE
read -p "Enter your AWS Default Region: " AWS_DEFAULT_REGION
read -p "Enter the environment (e.g., dev, staging, production): " ENVIRONMENT
read -p "Enter the default Bucket Name: " BUCKET_NAME
read -p "Enter the default Database User Name: " DB_USERNAME

# Export the environment variables immediately for the current session
export AWS_PROFILE="$AWS_PROFILE"
export AWS_DEFAULT_REGION="$AWS_DEFAULT_REGION"
export ENVIRONMENT="$ENVIRONMENT"
export BUCKET_NAME="$BUCKET_NAME"
export DB_USERNAME="$DB_USERNAME"

# Export Terraform-specific environment variables
export TF_VAR_AWS_PROFILE="$AWS_PROFILE"
export TF_VAR_AWS_DEFAULT_REGION="$AWS_DEFAULT_REGION"
export TF_VAR_ENVIRONMENT="$ENVIRONMENT"
export TF_VAR_BUCKET_NAME="$BUCKET_NAME"
export TF_VAR_DB_USERNAME="$DB_USERNAME"

# Append to ~/.bashrc to persist variables for future sessions
echo "export AWS_PROFILE=\"${AWS_PROFILE}\"" >> ~/.bashrc
echo "export AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION}\"" >> ~/.bashrc
echo "export ENVIRONMENT=\"${ENVIRONMENT}\"" >> ~/.bashrc
echo "export BUCKET_NAME=\"${BUCKET_NAME}\"" >> ~/.bashrc
echo "export DB_USERNAME=\"${DB_USERNAME}\"" >> ~/.bashrc
echo "export TF_VAR_AWS_PROFILE=\"${AWS_PROFILE}\"" >> ~/.bashrc
echo "export TF_VAR_AWS_DEFAULT_REGION=\"${AWS_DEFAULT_REGION}\"" >> ~/.bashrc
echo "export TF_VAR_ENVIRONMENT=\"${ENVIRONMENT}\"" >> ~/.bashrc
echo "export TF_VAR_BUCKET_NAME=\"${BUCKET_NAME}\"" >> ~/.bashrc
echo "export TF_VAR_DB_USERNAME=\"${DB_USERNAME}\"" >> ~/.bashrc

# Source the .bashrc file to make the changes take effect in the current session
source ~/.bashrc

echo "Environment variables have been configured and persisted to .bashrc!"
```

## Source Directory

### src/data-source/csv/creditCard.csv
```csv
index;category;amt;gender;city;state;city_pop;job;unix_time
0;misc_net;4.97;F;Moravian Falls;NC;3495;Psychologist counselling;1325376018
1;grocery_pos;107.23;F;Orient;WA;149;Special educational needs teacher;1325376044
2;entertainment;220.11;M;Malad City;ID;4154;Nature conservation officer;1325376051
3;gas_transport;45;M;Boulder;MT;1939;Patent attorney;1325376076
4;misc_pos;41.96;M;Doe Hill;VA;99;Dance movement psychotherapist;1325376186
5;gas_transport;94.63;F;Dublin;PA;2158;Transport planner;1325376248
6;grocery_net;44.54;F;Holcomb;KS;2691;Arboriculturist;1325376282
7;gas_transport;71.65;M;Edinburg;VA;6018;Designer multimedia;1325376308
8;misc_pos;4.27;F;Manor;PA;1472;Public affairs consultant;1325376318
9;grocery_pos;198.39;F;Clarksville;TN;151785;Pathologist;1325376361
10;grocery_pos;24.74;M;Clarinda;IA;7297;IT trainer;1325376383
```

### Image file: src/data-source/img/beach.jpg
This file is an image and cannot be displayed in text format.

### src/handler.py
```python
import json
import urllib.parse

from aws_lambda_powertools import Logger
from aws_lambda_powertools.utilities.typing import LambdaContext

from helpers.aws_wrangler import AWSWrangler
from helpers.db_conn import DBConn
from helpers.transform import transform

logger = Logger()
logger._logger.propagate = False

db = DBConn()
wr = AWSWrangler(logger, db)

@logger.inject_lambda_context()
def etl(event: dict, context: LambdaContext) -> str:
    
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = urllib.parse.unquote_plus(event['Records'][0]['s3']['object']['key'], encoding='utf-8')
    logger.append_keys(bucket=bucket, key=key)

    df = wr.read_csv(f"s3://{bucket}/{key}") # Extract
    df = transform(df) # Transform
    wr.insert_db(df, "credit_card_data") # Load

    return_body = {
        "message": "Go Serverless v4.0! Your function executed successfully!"
    }

    logger.info("Function executed successfully!")
    return {"statusCode": 200, "body": json.dumps(return_body)}
```

### src/helpers/aws_wrangler.py
```python
import awswrangler as wr

class AWSWrangler:
    
    def __init__(self, logger, db_conn):
        self.wrangler = wr
        self.logger = logger
        self.db_conn = db_conn  # Accepting a DBConn instance

    def read_csv(self, path):
        if path.split('.')[-1] != 'csv':
            raise ValueError('Only CSV files are supported')
        
        return self.wrangler.s3.read_csv(path, sep=";") # TODO: Add sep handler

    def insert_db(self, df, table_name):
        """Insert data into the MySQL database and close connection afterward."""
        with self.db_conn.get_conn() as con_mysql:  # Connection is managed automatically
            wr.mysql.to_sql(df, con_mysql, schema=self.db_conn.credentials['db_name'], table=table_name, mode="overwrite")
```

### src/helpers/boto3_client.py
```python
import os
import boto3

class Boto3Client:

    def __init__(self):        
        self.base_session = self._base_session()

    def _base_session(self):
        
        if os.environ.get("AWS_PROFILE") is None:
            return boto3.Session(region_name="us-east-1")
        
        else:
            return boto3.Session(
                    profile_name=os.environ.get("AWS_PROFILE"),
                    region_name="us-east-1")
    
    def define_secrets_client(self):
        return self.base_session.client(service_name="secretsmanager")

boto3_client = Boto3Client()
```

### src/helpers/db_conn.py
```python
import os
import json
import pymysql
from contextlib import contextmanager

from helpers.boto3_client import boto3_client

class DBConn:

    def __init__(self):
        self.credentials = self.get_credentials()
        pass  # We no longer open the connection here; it will be managed in the context

    def get_credentials(self):
        secrets_client = boto3_client.define_secrets_client()
        response = secrets_client.get_secret_value(SecretId=os.environ.get("RDS_SECRETS_ARN"))['SecretString']
        return json.loads(response)

    @contextmanager
    def get_conn(self):
        """Context manager to open and close a DB connection.""" 
        
        conn = pymysql.connect(
            host=os.environ.get("RDS_ENDPOINT").split(':')[0],
            user=self.credentials['username'],
            password=self.credentials['password'],
            db=self.credentials['db_name'],
            port=3306
        )

        try:
            yield conn  # Connection is open and passed to the caller
        finally:
            conn.close()  # Ensure the connection is closed after the operation
```

### src/helpers/transform.py
```python
def transform(df):
    """Apply transformations to the DataFrame."""
    df = df.drop(columns=['index'])
    return df
```

### src/package-lock.json
```json
{
  "name": "src",
  "version": "1.0.0",
  "lockfileVersion": 3,
  "requires": true,
  "packages": {
    "": {
      "name": "src",
      "version": "1.0.0",
      "license": "ISC",
      "devDependencies": {
        "serverless-deployment-bucket": "^1.6.0",
        "serverless-iam-roles-per-function": "^3.2.0"
      }
    }
  }
}
```

### src/package.json
```json
{
  "name": "src",
  "version": "1.0.0",
  "description": "<!-- title: 'AWS Python Example' description: 'This template demonstrates how to deploy a Python function running on AWS Lambda using the Serverless Framework.' layout: Doc framework: v4 platform: AWS language: python priority: 2 authorLink: 'https://github.com/serverless' authorName: 'Serverless, Inc.' authorAvatar: 'https://avatars1.githubusercontent.com/u/13742415?s=200&v=4' -->",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "author": "",
  "license": "ISC",
  "devDependencies": {
    "serverless-deployment-bucket": "^1.6.0",
    "serverless-iam-roles-per-function": "^3.2.0"
  }
}
```

### src/serverless.yml
```yaml
# "service" is the name of this project. This will also be added to your AWS resource names.
service: etl

frameworkVersion: '4'

provider:
  name: aws
  runtime: python3.12
  stage: ${opt:stage, 'dev'}
  memorySize: 512
  timeout: 30
  deploymentBucket:
    name: de-challenge-sls-deployment-bucket
    serverSideEncryption: AES256
  environment:
    BUCKET_NAME: ${env:BUCKET_NAME}
    STAGE: ${env:ENVIRONMENT}
    POWERTOOLS_SERVICE_NAME: etl
    POWERTOOLS_LOG_LEVEL: INFO
    RDS_SECRETS_ARN: ${env:RDS_SECRETS_ARN}
    RDS_ENDPOINT: ${env:RDS_ENDPOINT}

package:
  individually: true
  patterns:
    - '!data-source/**'
    - '!node_modules/**'

functions:
  etl:
    handler: handler.etl
    layers:
      - arn:aws:lambda:us-east-1:017000801446:layer:AWSLambdaPowertoolsPythonV3-python312-x86:1
      - arn:aws:lambda:us-east-1:336392948345:layer:AWSSDKPandas-Python312:13
      - { Ref: PymysqlLambdaLayer }
    events:
      - s3: # So lambda can be triggered when an object is created at S3 defined path
          bucket: ${self:provider.environment.BUCKET_NAME}
          event: s3:ObjectCreated:*
          rules:
            - prefix: data-source/
          existing: true
    
    iamRoleStatements:
      - Effect: Allow
        Action:
          - s3:GetObject
          - s3:ListBucket
        Resource:
          - "arn:aws:s3:::${self:provider.environment.BUCKET_NAME}/*"
          - "arn:aws:s3:::${self:provider.environment.BUCKET_NAME}"
      
      - Effect: Allow
        Action:
          - secretsmanager:GetSecretValue
        Resource:
          - ${self:provider.environment.RDS_SECRETS_ARN}

layers:
  pymysql:
    path: layers/pymysql
    compatibleRuntimes:
      - python3.12

plugins:
  - serverless-deployment-bucket # Allow always deploy at the same bucket
  - serverless-iam-roles-per-function # Allow creating IAM roles per function
```

### src/upload_to_s3.sh
```bash
#!/bin/bash

# Path to the file to be uploaded
FILE_PATH=$1

# S3 bucket and path
S3_BUCKET="s3://$BUCKET_NAME/data-source/"

# Upload the file to S3
echo "Uploading '$FILE_PATH' to '$S3_BUCKET'..."
aws s3 cp "$FILE_PATH" "$S3_BUCKET"

# Check if the upload was successful
if [ $? -eq 0 ]; then
  echo "File '$FILE_PATH' successfully uploaded to '$S3_BUCKET'."
else
  echo "Error: Failed to upload '$FILE_PATH' to S3."
  exit 1
fi
```

## Terraform Directory

### terraform/data-sources.tf
```hcl
# Importing list of AZs
data "aws_availability_zones" "aws_az" {
  state = "available"
}
```

### terraform/local.tf
```hcl
# Declare variables for AWS credentials
variable "aws_default_region" {}
variable "aws_profile" {}
variable "environment" {}
variable "bucket_name" {}
variable "db_username" {}

locals {

    aws_default_region  = var.aws_default_region
    aws_profile = var.aws_profile
    environment = var.environment

    # Use this local for bucket name
    bucket_name = var.bucket_name

    # Use this local for database name
    db_username = var.db_username

    # Use this local for secrets name
    secrets_name = "rds-secret-de-challenge"
    
    # Network variables defined as locals
    cidr_block_vpc              = "10.0.0.0/16"
    cidr_block_public_subnet_a   = "10.0.1.0/24"
    cidr_block_public_subnet_b   = "10.0.2.0/24"
    cidr_block_public_route_table = "0.0.0.0/0"    

    common_tags = {        
        "Environment" = var.environment
        "Owner" = "arthurmf"
        "Project" = "data-engineering-assessment"
    }    
}
```

This concludes the overview of the project files.
``` 

The above overview includes a comprehensive description and content of every file from the specified repository. It is organized in a clear and readable manner, suitable for understanding the structure and purpose of the project's key elements.